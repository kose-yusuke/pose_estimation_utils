import cv2
import numpy as np
import os

# ArUco ãƒãƒ¼ã‚«ãƒ¼ã®è¾æ›¸ã‚’å®šç¾©
aruco_dict = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_4X4_50)
aruco_params = cv2.aruco.DetectorParameters()

# ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨å‹•ç”»ã®ãƒ‘ã‚¹
video_paths = [
    "calib_mov/Record_QRCalibration_new_WebcamFullLeft.avi",
    "calib_mov/Record_QRCalibration_new_WebcamFullRight.avi",
    "calib_mov/Record_QRCalibration_new_WebcamLeft.avi",
    "calib_mov/Record_QRCalibration_new_WebcamRight.avi"
]

# ArUcoãƒãƒ¼ã‚«ãƒ¼ã®ã‚µã‚¤ã‚ºï¼ˆãƒ¡ãƒ¼ãƒˆãƒ«å˜ä½ï¼‰
marker_length = 0.3  # 30cm

# å„ã‚«ãƒ¡ãƒ©ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¿å­˜
camera_matrices = []
distortion_coeffs = []
rvecs_list = []
tvecs_list = []

for video_path in video_paths:
    cap = cv2.VideoCapture(video_path)

    objpoints = []  # 3Dç‚¹ãƒªã‚¹ãƒˆ
    imgpoints = []  # 2Dç‚¹ãƒªã‚¹ãƒˆ
    frame_shape = None  # ç”»åƒã‚µã‚¤ã‚ºã‚’ä¿å­˜
    frame_count = 0

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        frame_count += 1
        if frame_count % 10 != 0:  # 10ãƒ•ãƒ¬ãƒ¼ãƒ ã”ã¨ã«å‡¦ç†
            continue

        frame_shape = frame.shape[:2]  # ãƒ•ãƒ¬ãƒ¼ãƒ ã‚µã‚¤ã‚ºã‚’ä¿å­˜
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        # ArUcoãƒãƒ¼ã‚«ãƒ¼ã‚’æ¤œå‡ºï¼ˆROIãªã—ï¼‰
        corners, ids, _ = cv2.aruco.detectMarkers(gray, aruco_dict, parameters=aruco_params)

        # ãƒ‡ãƒãƒƒã‚°ç”¨: 50ãƒ•ãƒ¬ãƒ¼ãƒ ã”ã¨ã«ç”»åƒã‚’ä¿å­˜
        if frame_count % 50 == 0:
            cv2.imwrite(f"debug_frame_{frame_count}.jpg", gray)

        if ids is not None and len(corners) > 0:
            for i in range(len(ids)):
                imgpoints.append(corners[i].reshape(-1, 2))

                # ArUcoãƒãƒ¼ã‚«ãƒ¼ã®3Dåº§æ¨™ï¼ˆãƒ¯ãƒ¼ãƒ«ãƒ‰åº§æ¨™ç³»ï¼‰
                objp = np.array([
                    [-marker_length / 2,  marker_length / 2, 0],  # å·¦ä¸Š
                    [ marker_length / 2,  marker_length / 2, 0],  # å³ä¸Š
                    [ marker_length / 2, -marker_length / 2, 0],  # å³ä¸‹
                    [-marker_length / 2, -marker_length / 2, 0]   # å·¦ä¸‹
                ], dtype=np.float32)

                objpoints.append(objp)

    cap.release()

    if frame_shape is None:
        print(f"âŒ {video_path} ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãŒæ­£ã—ãèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸ")
        continue

    if len(objpoints) == 0 or len(imgpoints) == 0:
        print(f"âŒ {video_path}: ArUcoãƒãƒ¼ã‚«ãƒ¼ãŒæ¤œå‡ºã•ã‚Œãšã€ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãŒã§ãã¾ã›ã‚“")
        continue

    # ğŸ”¹ ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã®ãƒ‡ãƒ¼ã‚¿æ•°ã‚’åˆ¶é™
    if len(objpoints) > 100:
        objpoints = objpoints[:100]
        imgpoints = imgpoints[:100]

    assert len(objpoints) == len(imgpoints), f"âŒ {video_path}: objpoints ã¨ imgpoints ã®ã‚µã‚¤ã‚ºãŒä¸€è‡´ã—ã¾ã›ã‚“"

    # âœ… å†…éƒ¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨ˆç®—
    print(f"â³ {video_path}: ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œä¸­...")
    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, frame_shape[::-1], None, None)

    camera_matrices.append(mtx)
    distortion_coeffs.append(dist)
    rvecs_list.append(rvecs)
    tvecs_list.append(tvecs)

    print(f"âœ… ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æˆåŠŸ: {video_path}")

# ã‚«ãƒ¡ãƒ©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¿å­˜
np.savez("camera_calibration_params.npz",
         camera_matrices=camera_matrices,
         distortion_coeffs=distortion_coeffs,
         rvecs_list=rvecs_list,
         tvecs_list=tvecs_list)

print("âœ… å…¨ã‚«ãƒ¡ãƒ©ã®ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Œäº†ï¼")

import cv2
import json
import torch
import numpy as np
from mmpose.apis import init_pose_model, inference_top_down_pose_model
from torchvision import models
import torchvision.transforms as T

# âœ… **ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã® DeepLabV3 ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰**
segmentation_model = models.segmentation.deeplabv3_resnet101(pretrained=True).eval()

# âœ… **ViTPose ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰**
config_file = 'configs/body/2d_kpt_sview_rgb_img/topdown_heatmap/coco/ViTPose_huge_simple_coco_256x192.py'
checkpoint_file = 'pretrained_models/vitpose-h-simple.pth'
device = 'cuda' if torch.cuda.is_available() else 'cpu'
pose_model = init_pose_model(config_file, checkpoint_file, device=device)

# âœ… **å„ã‚«ãƒ¡ãƒ©ã®å‹•ç”»ã¨ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹JSONã®ãƒ‘ã‚¹**
video_paths = {
    "cam1": "demo/resources/3409_WebcamFullLeft_trial0_IDsFiltered_clean.mp4",
    "cam2": "demo/resources/3409_WebcamFullRight_trial0_IDsFiltered_clean.mp4",
    "cam3": "demo/resources/3409_WebcamLeft_trial0_IDsFiltered_clean.mp4",
    "cam4": "demo/resources/3409_WebcamRight_trial0_IDsFiltered_clean.mp4"
}

json_paths = {
    "cam1": "test_json/3409_WebcamFullLeft_trial0_IDsFiltered_clean.json",
    "cam2": "test_json/3409_WebcamFullRight_trial0_IDsFiltered_clean.json",
    "cam3": "test_json/3409_WebcamLeft_trial0_IDsFiltered_clean.json",
    "cam4": "test_json/3409_WebcamRight_trial0_IDsFiltered_clean.json"
}

# âœ… **ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã®å¤‰æ›é–¢æ•°**
transform = T.Compose([
    T.ToTensor(),
    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# âœ… **ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã®ä¿å­˜ç”¨è¾æ›¸**
keypoints_2d = {cam: [] for cam in video_paths.keys()}

# âœ… **å„ã‚«ãƒ¡ãƒ©ã®å‹•ç”»ã‚’å‡¦ç†**
for cam_id, (cam_name, video_path) in enumerate(video_paths.items()):
    print(f"ğŸ”µ {cam_name} ã®å‡¦ç†ã‚’é–‹å§‹...")

    cap = cv2.VideoCapture(video_path)

    # JSON ã®ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ï¼‰
    json_path = json_paths[cam_name]
    with open(json_path, "r") as f:
        bbox_data = json.load(f)

    frame_idx = 0
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        frame_idx_str = str(frame_idx)
        if frame_idx_str in bbox_data:
            bbox_list = bbox_data[frame_idx_str]
            bboxes = [{"bbox": entry["body_xy"]} for entry in bbox_list]
        else:
            bboxes = []

        # âœ… **é»’èƒŒæ™¯ã®ç”»åƒã‚’ä½œæˆ**
        black_background = np.zeros_like(frame, dtype=np.uint8)

        for bbox in bboxes:
            x1, y1, x2, y2 = bbox["bbox"]

            # **ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã®é ˜åŸŸã‚’åˆ‡ã‚Šå‡ºã—**
            bbox_frame = frame[y1:y2, x1:x2]

            # **ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é©ç”¨**
            bbox_rgb = cv2.cvtColor(bbox_frame, cv2.COLOR_BGR2RGB)
            input_tensor = transform(bbox_rgb).unsqueeze(0)

            with torch.no_grad():
                output = segmentation_model(input_tensor)["out"][0]

            # **ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³çµæœã‹ã‚‰ã€Œäººç‰©ï¼ˆã‚¯ãƒ©ã‚¹ID 15ï¼‰ã€ã®ã¿æŠ½å‡º**
            seg_map = output.argmax(0).byte().cpu().numpy()
            person_mask = (seg_map == 15).astype(np.uint8) * 255  # äººç‰©éƒ¨åˆ†ã¯ç™½ã€ãã‚Œä»¥å¤–ã¯é»’

            # **ãƒã‚¹ã‚¯ã‚’å…ƒã®ç”»åƒã«é©ç”¨ï¼ˆäººç‰©éƒ¨åˆ†ã®ã¿æ®‹ã™ï¼‰**
            bbox_segmented = cv2.bitwise_and(bbox_frame, bbox_frame, mask=person_mask)

            # **å…ƒã®ãƒ•ãƒ¬ãƒ¼ãƒ ã«é©ç”¨ï¼ˆé»’èƒŒæ™¯ã®ä¸Šã«ãƒšãƒ¼ã‚¹ãƒˆï¼‰**
            black_background[y1:y2, x1:x2] = bbox_segmented

        # âœ… **ViTPose ã§æ¨è«–ï¼ˆã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æ¸ˆã¿ç”»åƒã‚’ä½¿ç”¨ï¼‰**
        results = inference_top_down_pose_model(pose_model, black_background, bboxes)

        # âœ… **ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã®ä¿å­˜**
        if isinstance(results, tuple):
            results = results[0]

        frame_keypoints = []
        if isinstance(results, list):
            for res in results:
                if isinstance(res, dict) and "keypoints" in res:
                    keypoints = res["keypoints"]
                    frame_keypoints.append(keypoints.tolist())

        # âœ… ä¿®æ­£ï¼šãƒªã‚¹ãƒˆã®ãƒã‚¹ãƒˆã‚’å‰Šé™¤
        if frame_keypoints:
            keypoints_2d[cam_name].append(frame_keypoints[0])  # ä½™è¨ˆãªãƒã‚¹ãƒˆã‚’å‰Šé™¤
        else:
            keypoints_2d[cam_name].append([])

        frame_idx += 1

    cap.release()
    print(f"âœ… {cam_name} ã®å‡¦ç†ãŒå®Œäº†ï¼ï¼ˆç·ãƒ•ãƒ¬ãƒ¼ãƒ æ•°: {frame_idx}ï¼‰")

# âœ… **2Dã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã‚’JSONã«ä¿å­˜**
with open("keypoints_2d.json", "w") as f:
    json.dump(keypoints_2d, f, indent=4)

print("âœ… 4è¦–ç‚¹ã®2Dã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜å®Œäº†ï¼")
